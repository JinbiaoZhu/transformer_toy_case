{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c167da67-4b26-4db1-aaa7-258aaea90695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Literal, Tuple\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fee3f8-a1b8-466b-b419-c6ecbd74c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, ByteTensor, optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6117e01b-aed2-4f5f-b647-b4b500694e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e2694-81bb-4138-a10c-e467bc8faead",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 超参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c75a4ff-7238-4954-bb25-3b497ff3fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 超参数部分, 后面的参数名尽量与这部分保持一致\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # GPU配置\n",
    "dtype = torch.float32  # 数据类型配置: 这里默认是 32 位浮点数\n",
    "batch_size = 128  # 训练批次, 一批训练数据内包含句子的数量\n",
    "max_len = 128  # 单句最大长度\n",
    "d_model = 512  # 词嵌入向量维度\n",
    "n_layers = 3  # 编码层/解码层数量\n",
    "n_heads = 8  # 注意力头数: d_model = 512 / n_heads = 8 => 单头向量维度 64 , 即每个头 QKV 维度\n",
    "ffn_hidden = 2048  # 前向传播维度: 一般是词嵌入向量维度 d_model 的 4 倍数\n",
    "d_proj = ffn_hidden  # 跟前向传播维度一样\n",
    "n_hidden = ffn_hidden  # 跟前向传播维度一样\n",
    "drop_prob = 0.1  # dropout 提升鲁棒性，随机失活一些节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ce8286-2e1c-41f0-ac85-ae3d8421370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器超参数设置\n",
    "init_lr = 5e-6\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 150\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ad61bb-e897-4cc8-a826-27656bb44b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集制作的参数设置\n",
    "n = 5000  # 生成的字符串数量\n",
    "l = max_len  # 字符串的最大长度\n",
    "src = \"./datasets/src_string.txt\"\n",
    "trg = \"./datasets/trg_string.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f68f76-9fc5-444d-99ad-d2b48d9496cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot_name = \"results_ignore_padding_index_5e_6.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc8016-bfd6-4554-9c07-b6464fce02b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 分词器设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b091614-f715-45dd-9750-ef3816414105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模仿 huggingface transformers 的分词器编写一个类\n",
    "class Tokenizer:\n",
    "    def __init__(self, max_len, sos_token, padding_token, eos_token, device):\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.sos_token = sos_token\n",
    "        self.padding_token = padding_token\n",
    "        self.eos_token = eos_token\n",
    "        self.device = device\n",
    "\n",
    "        self.VOCABULARY = []\n",
    "        self.token2index = {}\n",
    "        self.index2token = {}\n",
    "        self.vocab = None\n",
    "\n",
    "        self.padding_index, self.sos_index, self.eod_index = -1, -1, -1\n",
    "\n",
    "    def from_pretrained(self, filepath: str):\n",
    "\n",
    "        with open(filepath, 'r') as fp:\n",
    "            for line in fp:\n",
    "                self.VOCABULARY.append(line.strip())\n",
    "        fp.close()\n",
    "\n",
    "        self.vocab = len(self.VOCABULARY)\n",
    "\n",
    "        for v, index in zip(self.VOCABULARY, range(len(self.VOCABULARY))):\n",
    "            self.token2index[v] = index\n",
    "\n",
    "        for v, index in zip(self.VOCABULARY, range(len(self.VOCABULARY))):\n",
    "            self.index2token[index] = v\n",
    "\n",
    "        self.padding_index = self.token2index[self.padding_token]\n",
    "        self.sos_index = self.token2index[self.sos_token]\n",
    "        self.eos_index = self.token2index[self.eos_token]\n",
    "\n",
    "    def encode(self, sentences, return_tensor=False):\n",
    "        encode_list = []\n",
    "        for sentence in sentences:\n",
    "\n",
    "            if len(sentence) <= self.max_len - 2:\n",
    "                encode_list.append(\n",
    "                    [self.token2index[self.sos_token]] +\n",
    "                    [self.token2index[char] for char in sentence] +\n",
    "                    [self.token2index[self.eos_token]] +\n",
    "                    [self.token2index[self.padding_token]] * (self.max_len - 2 - len(sentence))\n",
    "                )\n",
    "            else:\n",
    "                encode_list.append(\n",
    "                    [self.token2index[self.sos_token]] +\n",
    "                    [self.token2index[char] for char in sentence[0:self.max_len - 2]] +\n",
    "                    [self.token2index[self.eos_token]]\n",
    "                )\n",
    "\n",
    "        if return_tensor:\n",
    "            # encode_list = torch.tensor(encode_list).to(device=self.device, dtype=torch.int)\n",
    "            encode_list = torch.tensor(encode_list).to(device=self.device)\n",
    "\n",
    "        return encode_list\n",
    "\n",
    "    def decode(self, sentences):\n",
    "        decode_list = []\n",
    "\n",
    "        if not isinstance(sentences[0], list):\n",
    "            # 解码过程中, 如果只有单列表, 例如 [1, 2, 3] 则需要额外嵌套一层列表\n",
    "            # 默认的解码都是一个批次的, 因此是双层列表嵌套\n",
    "            sentences = [sentences]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            decode_list.append([self.index2token[deco] for deco in sentence])\n",
    "        return decode_list\n",
    "\n",
    "    def generate(self, tokenList: Literal[Literal]):\n",
    "\n",
    "        def single(tokenList):\n",
    "            str_to_return = ''\n",
    "            for char in tokenList:\n",
    "                str_to_return += char\n",
    "            return str_to_return\n",
    "\n",
    "        return [single(token_list) for token_list in tokenList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75561ef2-5cd5-42c5-9e54-a2abd89c4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(max_len=max_len, sos_token='$', padding_token='&', eos_token='#', device=device)\n",
    "tokenizer.from_pretrained(\"./VOCABULARY.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea17f37f-d19d-4c1b-8acf-2031bbd9919e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 53)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "padding_idx = tokenizer.padding_index  # padding token 的序列号\n",
    "(vocab, padding_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a005894-f6fb-430a-8839-aa04112e650a",
   "metadata": {},
   "source": [
    "### 模型设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19dd15d9-bf0d-46e8-af63-dfcbdd8faf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 嵌入部分\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, max_len, d_model, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        1. 嵌入部分包括词嵌入和位置编码, 二者相加 -> dropout -> 作为编码器或解码器的输入。\n",
    "        2. 细节: 词嵌入会设置导数, 位置编码的索引张量不设置导数\n",
    "        :param vocab: 单词表的数量\n",
    "        :param max_len: 一句话的最大 token 长度\n",
    "        :param d_model: 词嵌入向量维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 张量存放设备\n",
    "        :param dtype: 张量数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 词嵌入部分\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=self.vocab, embedding_dim=self.d_model)\n",
    "\n",
    "        # 位置编码部分: (max_len, d_model) 的二维张量, 不需要导数\n",
    "        self.position_embedding_map = torch.zeros(size=(self.max_len, self.d_model))\n",
    "        self.position_embedding_map.requires_grad = False\n",
    "        # 设置奇数维度和偶数维度的索引列表\n",
    "        odd, even = torch.arange(1, self.d_model, 2), torch.arange(0, self.d_model, 2)\n",
    "        # 设置一个句子 token 的全部索引\n",
    "        # 这里 \"unsqueeze(1)\" 的作用是让句子 token 的位置索引可以广播\n",
    "        ########################################################################################\n",
    "        # 回忆: pytorch 的张量基本运算是按元素位置计算的, 因此两个相同 shape 的张量结果返回的也是相同 shape\n",
    "        # 例如: 两个张量的维度都是 (4.) , 进行基本运算的结果就是 (4.)\n",
    "        # 例如: 一个张量的维度是 (4.) , 另一个维度是 (4, 1) 那么 (4, 1) 会广播成 (4, 4) 再做运算\n",
    "        # 分析: pos 的原本张量维度是 (self.max_len.) , even 和 odd 都是 (256.) 直接做运算会报错\n",
    "        # 分析: 原本情况 pos 和 even 都被视为向量, 但是二者维度不匹配, 因此无法计算\n",
    "        # 分析: pos 最后扩充一个维度时, (self.max_len, 1) , pos 会在最后这个 1 维度广播机制重复 256 次\n",
    "        # 分析: 再将广播机制后的 (self.max_len, 256) 与 even 或者 odd 做运算\n",
    "        ########################################################################################\n",
    "        pos = torch.arange(0, self.max_len, 1).unsqueeze(1)\n",
    "        # 根据 odd, even 和 pos 填充 self.position_embedding_map 张量内的元素\n",
    "        self.position_embedding_map[:, even] = torch.sin(pos / (1e4 ** (even / self.d_model)))\n",
    "        self.position_embedding_map[:, odd] = torch.cos(pos / (1e4 ** (even / self.d_model)))\n",
    "\n",
    "        # 正则化部分\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # 对初始化好的 self.word_embedding 和 self.position_embedding_map 进行数据类型和设备设置\n",
    "        self.word_embedding.to(device=self.device, dtype=self.dtype)\n",
    "        self.position_embedding_map.to(device=self.device, dtype=self.dtype)\n",
    "        self.dropout.to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        对输入的离散稀疏 token 的编号计算得到连续稠密 embedding 向量\n",
    "        :param x: 输入 tokens 的编号序列\n",
    "        :return: 输出 embedding 张量\n",
    "        \"\"\"\n",
    "        # batch_size 一批训练数据内包含句子的数量\n",
    "        # max_len 一批训练数据内单句最大长度, 且 max_len <= 位置编码初始化设定的长度\n",
    "        batch_size, max_len = x.shape\n",
    "\n",
    "        # 词嵌入\n",
    "        word_embedding = self.word_embedding(x)\n",
    "        # 位置编码\n",
    "        # debug: 最初维度要扩充一个维度, 以实现广播机制\n",
    "        # debug: position_encoding 存储在 cpu 上, 还要将其转到 cuda 上\n",
    "        position_encode = self.position_embedding_map[:max_len, :].unsqueeze(0).to(device=self.device, dtype=self.dtype)\n",
    "        # 两者相加再 dropout 正则化\n",
    "        encode = self.dropout(word_embedding + position_encode)\n",
    "\n",
    "        return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2dfc20-fe2b-4a6e-939b-fb814cc5ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 层归一化算法\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, dtype, device, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        :param d_model: 模型嵌入维度\n",
    "        :param dtype: 数据类型\n",
    "        :param device: 显卡设备\n",
    "        :param epsilon: 设置的一个很小很小的数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.epsilon = epsilon\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        # 初始化可以更新的全 0 全 1 张量参数\n",
    "        self.A = nn.Parameter(torch.ones(self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "        self.B = nn.Parameter(torch.zeros(self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. 不计算无偏的, 也就是方差公式除以 N 而不是 N-1\n",
    "        # 2. keepdim=True, 保留原有维度, 便于进行广播机制\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "        x = self.A * (x - mean) / (var + self.epsilon) + self.B\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8242231b-c501-4300-9333-6a76f2eb4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 编码器部分\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, max_len, d_model, n_heads, d_proj, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        :param max_len: 单句最大长度\n",
    "        :param d_model: 词嵌入向量维度\n",
    "        :param n_heads: 注意力头数\n",
    "        :param d_proj: 投影层的维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 张量存放设备\n",
    "        :param dtype: 张量数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_proj = d_proj\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 计算出每个头的子空间维度\n",
    "        self.d_per_head = int(self.d_model / self.n_heads)\n",
    "\n",
    "        # 初始化 query key value 的线性投射层\n",
    "        self.Wq = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wk = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wv = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化头合并的投射层, 以及前馈神经网络 FFNN\n",
    "        self.Wc = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wf = nn.Sequential(nn.Linear(self.d_model, self.d_proj),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(self.d_proj, self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化正则化\n",
    "        self.Dropout = nn.Dropout(self.dropout).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化层归一化\n",
    "        self.layerNorm = LayerNorm(self.d_model, self.dtype, self.device)\n",
    "\n",
    "        # 对所有线性投射层的权重进行恺明初始化\n",
    "        torch.nn.init.kaiming_uniform_(self.Wq.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wk.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wv.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wc.weight)\n",
    "        for layer in self.Wf:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, input_content: Tuple[Tensor, ByteTensor]):\n",
    "        \"\"\"\n",
    "        :param input_content: 输入内容的元组, 包含两个元素: 一个是输入张量, 另一个是注意力掩码\n",
    "        \"\"\"\n",
    "        x, mask = input_content\n",
    "\n",
    "        # batch_size 一个批次的句子数量\n",
    "        # max_len 句子的最大 token 数量\n",
    "        # d_model 嵌入维度\n",
    "        batch_size, max_len, d_model = x.shape\n",
    "\n",
    "        # x 分别当做 query key value 输入线性投射层\n",
    "        # x_q, x_k, x_v: (batch_size, max_len, d_model)\n",
    "        x_q, x_k, x_v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "\n",
    "        # 分头, 并将头分出来\n",
    "        # x_q, x_k, x_v: (batch_size, n_heads, max_len, d_per_head)\n",
    "        x_q = x_q.view(batch_size, max_len, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_k = x_k.view(batch_size, max_len, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_v = x_v.view(batch_size, max_len, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 注意力机制计算\n",
    "        # 做乘法注意力 temp: (batch_size, n_heads, max_len, max_len)\n",
    "        temp = x_q @ x_k.permute(0, 1, 3, 2)\n",
    "        # 除以缩放算子\n",
    "        temp /= math.sqrt(self.d_per_head)\n",
    "        ########################################################################################\n",
    "        # 其中 mask 必须是一个 ByteTensor, shape 必须和 a 一样, 且元素只能是 0 或者 1 .\n",
    "        # 将 mask 中为 1 的元素所在的索引, 在 a 中相同的的索引处替换为 value, mask value 必须同为 tensor\n",
    "        # 这里的 mask 被掩码的是 1, 没被掩码的是 0\n",
    "        ########################################################################################\n",
    "        # 进行自编码器掩码操作\n",
    "        temp.masked_fill(mask, -1 * torch.inf)\n",
    "        # 进行 softmax 归一化, dim=-1 表示只对最后的维度, 就是嵌入维度做归一化\n",
    "        # attention: (batch_size, n_heads, max_len, max_len)\n",
    "        attention = torch.softmax(temp, dim=-1)\n",
    "        # 将注意力分数乘以 value 张量\n",
    "        # value: (batch_size, n_heads, max_len, d_per_head)\n",
    "        value = attention @ x_v\n",
    "        # 将每个头合并\n",
    "        # 若在维度变换后还需要进行 reshape 操作的话, 需要在后面加 contiguous() 保持连续 \n",
    "        # total_value: (batch_size, max_len, d_model)\n",
    "        total_value = value.permute(0, 2, 1, 3).contiguous().reshape((batch_size, max_len, d_model))\n",
    "        # 输入头合并投影层, 输出表示自注意力模块结束\n",
    "        # total_value_: (batch_size, max_len, d_model)\n",
    "        total_value_ = self.Wc(total_value)\n",
    "        # 马上进行正则化\n",
    "        total_value_ = self.Dropout(total_value_)\n",
    "        # 进行残差连接和层归一化\n",
    "        last = self.layerNorm(x + total_value_)\n",
    "        return (last, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e29bb12-37b0-4b50-a3e8-0c7cfccce475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 解码器部分\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, max_len, d_model, n_heads, d_proj, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        :param max_len: 单句最大长度\n",
    "        :param d_model: 词嵌入向量维度\n",
    "        :param n_heads: 注意力头数\n",
    "        :param d_proj: 投影层的维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 张量存放设备\n",
    "        :param dtype: 张量数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_proj = d_proj\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 计算出每个头的子空间维度\n",
    "        self.d_per_head = int(self.d_model / self.n_heads)\n",
    "\n",
    "        # 初始化 query key value 的线性投射\n",
    "        self.Wq = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wk = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wv = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化头合并的投射以及前馈神经网络\n",
    "        self.Wc = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wf = nn.Sequential(nn.Linear(self.d_model, self.d_proj),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(self.d_proj, self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化正则化\n",
    "        self.Dropout = nn.Dropout(self.dropout).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化层归一化\n",
    "        self.layerNorm = LayerNorm(self.d_model, self.dtype, self.device)\n",
    "\n",
    "        # 对所有线性投射层的权重进行恺明初始化\n",
    "        torch.nn.init.kaiming_uniform_(self.Wq.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wk.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wv.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wc.weight)\n",
    "        for layer in self.Wf:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, input_content: Tuple[Tensor, Tensor, Tensor, ByteTensor, ByteTensor]):\n",
    "        \"\"\"\n",
    "        :param input_content: 输入内容的元组, 包含五个元素: 三个是 query key 和 value , 另外两个是解码注意力掩码和编码解码掩码\n",
    "        \"\"\"\n",
    "        q, k_, v_, cross_mask, decode_mask = input_content\n",
    "\n",
    "        k = copy.copy(k_)\n",
    "        v = copy.copy(v_)\n",
    "\n",
    "        batch_size_q, max_len_q, d_model_q = q.shape\n",
    "        batch_size_k, max_len_k, d_model_k = k.shape\n",
    "\n",
    "        # x 分别当做 query key value 输入线性投射层\n",
    "        # x_q, x_k, x_v: (batch_size, max_len, d_model)\n",
    "        x_q, x_k, x_v = self.Wq(q), self.Wk(q), self.Wv(q)\n",
    "\n",
    "        # 分头, 并将头分出来\n",
    "        # x_q, x_k, x_v: (batch_size, n_heads, max_len, d_per_head)\n",
    "        x_q = x_q.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_k = x_k.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_v = x_v.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 注意力机制计算\n",
    "        # 做乘法注意力 temp: (batch_size, n_heads, max_len, max_len)\n",
    "        temp = x_q @ x_k.permute(0, 1, 3, 2)\n",
    "        # 除以缩放算子\n",
    "        temp /= math.sqrt(self.d_per_head)\n",
    "        ########################################################################################\n",
    "        # 其中 mask 必须是一个 ByteTensor, shape 必须和 a 一样, 且元素只能是 0 或者 1 .\n",
    "        # 将 mask 中为 1 的元素所在的索引, 在 a 中相同的的索引处替换为 value, mask value 必须同为 tensor\n",
    "        # 这里的 mask 被掩码的是 1, 没被掩码的是 0\n",
    "        ########################################################################################\n",
    "        # 进行自编码器掩码操作\n",
    "        temp.masked_fill(decode_mask, -1 * torch.inf)\n",
    "        # 进行 softmax 归一化, dim=-1 表示只对最后的维度, 就是嵌入维度做归一化\n",
    "        # attention: (batch_size, n_heads, max_len, max_len)\n",
    "        attention = torch.softmax(temp, dim=-1)\n",
    "        # 将注意力分数乘以 value 张量\n",
    "        # value: (batch_size, n_heads, max_len, d_per_head)\n",
    "        value = attention @ x_v\n",
    "        # 将每个头合并\n",
    "        # total_value: (batch_size, max_len, d_model)\n",
    "        total_value = value.permute(0, 2, 1, 3).contiguous().reshape((batch_size_q, max_len_q, d_model))\n",
    "        # 输入头合并投影层, 输出表示自注意力模块结束\n",
    "        # total_value_: (batch_size, max_len, d_model)\n",
    "        total_value_ = self.Wc(total_value)\n",
    "        # 马上进行正则化\n",
    "        total_value_ = self.Dropout(total_value_)\n",
    "        # 进行残差连接和层归一化\n",
    "        last = self.layerNorm(q + total_value_)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        last_ = copy.copy(last)\n",
    "        last_ = last_.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch_size_k, max_len_k, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch_size_k, max_len_k, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 注意力机制计算\n",
    "        temp = last_ @ k.permute(0, 1, 3, 2)\n",
    "        # 除以缩放算子\n",
    "        temp /= math.sqrt(self.d_per_head)\n",
    "        # 进行编码-解码器掩码操作\n",
    "        temp.masked_fill(cross_mask, -1 * torch.inf)\n",
    "        # 进行 softmax 归一化\n",
    "        attention = torch.softmax(temp, dim=-1)\n",
    "        # 将注意力分数乘以 value 张量\n",
    "        value = attention @ v\n",
    "        # 将每个头合并\n",
    "        total_value = value.permute(0, 2, 1, 3).contiguous().reshape((batch_size_q, max_len_q, d_model))\n",
    "        # 输入头合并投影层, 输出表示互注意力模块结束\n",
    "        total_value_ = self.Wc(total_value)\n",
    "        # 马上进行正则化\n",
    "        total_value_ = self.Dropout(total_value_)\n",
    "        # 进行残差连接和层归一化\n",
    "        last_decode = self.layerNorm(last + total_value_)\n",
    "\n",
    "        return (last_decode, k_, v_, cross_mask, decode_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532ed498-999b-406d-b6d6-b53112077d79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 投射到词汇表\n",
    "class ProjVocab(nn.Module):\n",
    "    def __init__(self, vocab, d_model, d_proj, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        :param vocab: 词汇表数量\n",
    "        :param d_model: 模型嵌入维度\n",
    "        :param d_proj: 线性投射层维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 显卡设备\n",
    "        :param dtype: 数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.d_proj = d_proj\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.projVocab = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_proj),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_proj, self.vocab)\n",
    "        ).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        for layer in self.projVocab:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projVocab(x)\n",
    "        # softmax 归一化处理, 含义是 \"预测出来的, 在词汇表上每个 token 出现的概率\"\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9edb9d-795a-48ec-a6d9-88c025074302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 汇总成 Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_len, d_model, n_heads, n_layers, d_proj, vocab, dropout, device, dtype, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_proj = d_proj\n",
    "        self.vocab = vocab\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # 每个模块类实例的时候都进行了权重初始化, 设备和数据类型的声明\n",
    "        # 初始化单个编码器和解码器\n",
    "        single_encoder = Encoder(self.max_len, self.d_model, self.n_heads, self.d_proj, self.dropout, self.device,\n",
    "                                 self.dtype)\n",
    "        single_decoder = Decoder(self.max_len, self.d_model, self.n_heads, self.d_proj, self.dropout, self.device,\n",
    "                                 self.dtype)\n",
    "\n",
    "        # 初始化编码部分和解码部分的嵌入层\n",
    "        self.embedding = Embedding(self.vocab, self.max_len, self.d_model, self.dropout, self.device, self.dtype)\n",
    "\n",
    "        # 使用列表解包的方法构建整个网络\n",
    "        # 注意: nn.Sequential 构建的网络只允许单个变量输入模型中, 因此在编码器和解码器中进行了打包和解包操作\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[single_encoder for _ in range(self.n_layers)]\n",
    "        )\n",
    "        self.decoders = nn.Sequential(\n",
    "            *[single_decoder for _ in range(self.n_layers)]\n",
    "        )\n",
    "\n",
    "        # 初始化线性投射层\n",
    "        self.projVocab = ProjVocab(self.vocab, self.d_model, self.d_proj, self.dropout, self.device, self.dtype)\n",
    "\n",
    "    def forward(self, src_seq, trg_seq):\n",
    "        \"\"\"\n",
    "        训练过程的 transformer 的前向推理\n",
    "        :param src_seq: 源序列\n",
    "        :param trg_seq: 目标序列\n",
    "        :return: 每个句子每个 token 的下一个预测的 token \n",
    "        \"\"\"\n",
    "        # 先获得编码器掩码, 解码器掩码和编码-解码掩码\n",
    "        self_mask = self.make_mask(src_seq, src_seq, \"encoder\")\n",
    "        cross_mask = self.make_mask(trg_seq, src_seq, \"encoder-decoder\")\n",
    "        decoder_mask = self.make_mask(trg_seq, trg_seq, \"decoder\")\n",
    "\n",
    "        # 计算源序列嵌入和目标序列嵌入\n",
    "        en_emb = self.embedding(src_seq)\n",
    "        de_emb = self.embedding(trg_seq)\n",
    "\n",
    "        # 将源序列嵌入和编码器掩码送入编码器计算源序列的编码信息\n",
    "        encodes_tuple = self.encoders((en_emb, self_mask))\n",
    "        # 从计算结果的包中得到编码\n",
    "        encodes, _ = encodes_tuple\n",
    "        # 将目标序列的嵌入, 两个编码 (分别做 key 和 value), 编码-解码掩码 和 解码器掩码送入解码器计算每句话每个 token 的下一个预测\n",
    "        decodes = self.decoders((de_emb, encodes, encodes, cross_mask, decoder_mask))\n",
    "        # 从计算结果的包中得到解码\n",
    "        last_decode, _, _, _, _ = decodes\n",
    "        # 将解码内容送入投射层中获得在词汇表中每个 token 的预测概率\n",
    "        vocab_pos = self.projVocab(last_decode)\n",
    "        return vocab_pos\n",
    "\n",
    "    def make_mask(self, q: Tensor, k: Tensor, type: Literal[\"encoder\", \"encoder-decoder\", \"decoder\"]):\n",
    "        \"\"\"\n",
    "        这个类方法用于构建编码器掩码, 解码器掩码和编码-解码掩码\n",
    "        :param q: query 张量\n",
    "        :param k: key 张量\n",
    "        :param type: 选择是编码器掩码, 解码器掩码 和编码-解码掩码\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        max_len_q = q.shape[1]  # 获得 query 和 key 的每一句话的最大 token 长度\n",
    "        max_len_k = k.shape[1]\n",
    "\n",
    "        # qMask: (batch_size, max_len_q)\n",
    "        qMask = q.ne(padding_idx)  # 过滤掉被 padding 的 token\n",
    "        # qMask: (batch_size, 1, max_len_q, 1)\n",
    "        qMask = qMask.unsqueeze(1).unsqueeze(3)\n",
    "        # qMask: (batch_size, 1, max_len_q, max_len_k)\n",
    "        qMask = qMask.repeat(1, 1, 1, max_len_k)\n",
    "\n",
    "        # kMask: (batch_size, max_len_k)\n",
    "        kMask = k.ne(padding_idx)\n",
    "        # kMask: (batch_size, 1, 1, max_len_k)\n",
    "        kMask = kMask.unsqueeze(1).unsqueeze(2)\n",
    "        # kMask: (batch_size, 1, max_len_q, max_len_k)\n",
    "        kMask = kMask.repeat(1, 1, max_len_q, 1)\n",
    "\n",
    "        Mask = qMask & kMask\n",
    "\n",
    "        # 如果是解码器注意力时, 需要设置一个同大小的下三角掩码, 然后做与运算\n",
    "        if type == \"decoder\":\n",
    "            trigl = torch.tril(torch.ones_like(Mask))\n",
    "            Mask &= trigl\n",
    "\n",
    "        return Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91a2436e-3519-40b9-98ac-a901560b54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(max_len=max_len,\n",
    "                          d_model=d_model,\n",
    "                          n_heads=n_heads,\n",
    "                          n_layers=n_layers,\n",
    "                          d_proj=d_proj,\n",
    "                          vocab=vocab,\n",
    "                          dropout=drop_prob,\n",
    "                          device=device,\n",
    "                          dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb30ae2-dc33-428c-989f-331d1836d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 7.49M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in transformer.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "036dd613-24b4-44e3-8314-d33855f7729b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(\n",
       "    (word_embedding): Embedding(55, 512)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoders): Sequential(\n",
       "    (0): Encoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (decoders): Sequential(\n",
       "    (0): Decoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (projVocab): ProjVocab(\n",
       "    (projVocab): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=55, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2618686-63a5-4f80-a3e0-3d3fb63f520c",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e11f6aa3-97df-4e99-ac01-80fae46a4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, num_gene: int, test_src: str, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "\n",
    "        test_src_index = tokenizer.encode(test_src, return_tensor=True)\n",
    "\n",
    "        gene_trg = tokenizer.sos_token\n",
    "\n",
    "        for _ in range(num_gene):\n",
    "            gene_trg_index = tokenizer.encode(gene_trg, return_tensor=True)\n",
    "    \n",
    "            output = model(test_src_index, gene_trg_index)\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "    \n",
    "            output_words = output.softmax(dim=-1).max(dim=-1)[1]\n",
    "            output_words = tokenizer.decode(output_words.data.cpu().numpy().tolist())\n",
    "            output_words = tokenizer.generate(output_words)[0]\n",
    "\n",
    "            gene_trg += output_words\n",
    "            \n",
    "            del gene_trg_index\n",
    "\n",
    "    return gene_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62a58cd1-ddc3-4990-a891-1515d9a0bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只能使用一次\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "278eb4c4-936b-485e-b765-a9d834cbc72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 7.49M\n",
      "Transformer(\n",
      "  (embedding): Embedding(\n",
      "    (word_embedding): Embedding(55, 512)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoders): Sequential(\n",
      "    (0): Encoder(\n",
      "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wf): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layerNorm): LayerNorm()\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wf): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layerNorm): LayerNorm()\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wf): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layerNorm): LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (decoders): Sequential(\n",
      "    (0): Decoder(\n",
      "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wf): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layerNorm): LayerNorm()\n",
      "    )\n",
      "    (1): Decoder(\n",
      "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wf): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layerNorm): LayerNorm()\n",
      "    )\n",
      "    (2): Decoder(\n",
      "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (Wf): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (Dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layerNorm): LayerNorm()\n",
      "    )\n",
      "  )\n",
      "  (projVocab): ProjVocab(\n",
      "    (projVocab): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=2048, out_features=55, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trained_device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "trained_model = torch.load(\"my_transformer.pth\").to(trained_device)\n",
    "total = sum([param.nelement() for param in trained_model.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29b1a127-fc1c-4bf0-a04b-1181b851dc91",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.75 GiB total capacity; 7.20 GiB already allocated; 499.88 MiB free; 9.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, num_gene, test_src, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_gene):\n\u001b[1;32m     11\u001b[0m     gene_trg_index \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(gene_trg, return_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_src_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_trg_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     output_reshape \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     16\u001b[0m     output_words \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMs/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[16], line 61\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src_seq, trg_seq)\u001b[0m\n\u001b[1;32m     59\u001b[0m encodes, _ \u001b[38;5;241m=\u001b[39m encodes_tuple\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 将目标序列的嵌入, 两个编码 (分别做 key 和 value), 编码-解码掩码 和 解码器掩码送入解码器计算每句话每个 token 的下一个预测\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m decodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mde_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# 从计算结果的包中得到解码\u001b[39;00m\n\u001b[1;32m     63\u001b[0m last_decode, _, _, _, _ \u001b[38;5;241m=\u001b[39m decodes\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMs/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMs/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LLMs/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[14], line 111\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input_content)\u001b[0m\n\u001b[1;32m    108\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(batch_size_k, max_len_k, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_per_head)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# 注意力机制计算\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mlast_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# 除以缩放算子\u001b[39;00m\n\u001b[1;32m    113\u001b[0m temp \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_per_head)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 962.00 MiB (GPU 0; 10.75 GiB total capacity; 7.20 GiB already allocated; 499.88 MiB free; 9.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(test(trained_model, 20, [\"apple\"], device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
