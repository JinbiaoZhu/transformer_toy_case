{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c167da67-4b26-4db1-aaa7-258aaea90695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from typing import Literal, Tuple\n",
    "import os\n",
    "import random\n",
    "import tqdm\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fee3f8-a1b8-466b-b419-c6ecbd74c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor, ByteTensor, optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6117e01b-aed2-4f5f-b647-b4b500694e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e2694-81bb-4138-a10c-e467bc8faead",
   "metadata": {},
   "source": [
    "### 超参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c75a4ff-7238-4954-bb25-3b497ff3fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 超参数部分, 后面的参数名尽量与这部分保持一致\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # GPU配置\n",
    "dtype = torch.float32  # 数据类型配置: 这里默认是 32 位浮点数\n",
    "batch_size = 128  # 训练批次, 一批训练数据内包含句子的数量\n",
    "max_len = 128  # 单句最大长度\n",
    "d_model = 512  # 词嵌入向量维度\n",
    "n_layers = 3  # 编码层/解码层数量\n",
    "n_heads = 8  # 注意力头数: d_model = 512 / n_heads = 8 => 单头向量维度 64 , 即每个头 QKV 维度\n",
    "ffn_hidden = 2048  # 前向传播维度: 一般是词嵌入向量维度 d_model 的 4 倍数\n",
    "d_proj = ffn_hidden  # 跟前向传播维度一样\n",
    "n_hidden = ffn_hidden  # 跟前向传播维度一样\n",
    "drop_prob = 0.1  # dropout 提升鲁棒性，随机失活一些节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ce8286-2e1c-41f0-ac85-ae3d8421370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器超参数设置\n",
    "init_lr = 5e-6\n",
    "factor = 0.9\n",
    "adam_eps = 5e-9\n",
    "patience = 10\n",
    "warmup = 100\n",
    "epoch = 150\n",
    "clip = 1.0\n",
    "weight_decay = 5e-4\n",
    "inf = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84ad61bb-e897-4cc8-a826-27656bb44b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集制作的参数设置\n",
    "n = 5000  # 生成的字符串数量\n",
    "l = max_len  # 字符串的最大长度\n",
    "src = \"./datasets/src_string.txt\"\n",
    "trg = \"./datasets/trg_string.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f68f76-9fc5-444d-99ad-d2b48d9496cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plot_name = \"results_ignore_padding_index_5e_6.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc8016-bfd6-4554-9c07-b6464fce02b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 分词器设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b091614-f715-45dd-9750-ef3816414105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模仿 huggingface transformers 的分词器编写一个类\n",
    "class Tokenizer:\n",
    "    def __init__(self, max_len, sos_token, padding_token, eos_token, device):\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.sos_token = sos_token\n",
    "        self.padding_token = padding_token\n",
    "        self.eos_token = eos_token\n",
    "        self.device = device\n",
    "\n",
    "        self.VOCABULARY = []\n",
    "        self.token2index = {}\n",
    "        self.index2token = {}\n",
    "        self.vocab = None\n",
    "\n",
    "        self.padding_index, self.sos_index, self.eod_index = -1, -1, -1\n",
    "\n",
    "    def from_pretrained(self, filepath: str):\n",
    "\n",
    "        with open(filepath, 'r') as fp:\n",
    "            for line in fp:\n",
    "                self.VOCABULARY.append(line.strip())\n",
    "        fp.close()\n",
    "\n",
    "        self.vocab = len(self.VOCABULARY)\n",
    "\n",
    "        for v, index in zip(self.VOCABULARY, range(len(self.VOCABULARY))):\n",
    "            self.token2index[v] = index\n",
    "\n",
    "        for v, index in zip(self.VOCABULARY, range(len(self.VOCABULARY))):\n",
    "            self.index2token[index] = v\n",
    "\n",
    "        self.padding_index = self.token2index[self.padding_token]\n",
    "        self.sos_index = self.token2index[self.sos_token]\n",
    "        self.eos_index = self.token2index[self.eos_token]\n",
    "\n",
    "    def encode(self, sentences, return_tensor=False):\n",
    "        encode_list = []\n",
    "        for sentence in sentences:\n",
    "\n",
    "            if len(sentence) <= self.max_len - 2:\n",
    "                encode_list.append(\n",
    "                    [self.token2index[self.sos_token]] +\n",
    "                    [self.token2index[char] for char in sentence] +\n",
    "                    [self.token2index[self.eos_token]] +\n",
    "                    [self.token2index[self.padding_token]] * (self.max_len - 2 - len(sentence))\n",
    "                )\n",
    "            else:\n",
    "                encode_list.append(\n",
    "                    [self.token2index[self.sos_token]] +\n",
    "                    [self.token2index[char] for char in sentence[0:self.max_len - 2]] +\n",
    "                    [self.token2index[self.eos_token]]\n",
    "                )\n",
    "\n",
    "        if return_tensor:\n",
    "            # encode_list = torch.tensor(encode_list).to(device=self.device, dtype=torch.int)\n",
    "            encode_list = torch.tensor(encode_list).to(device=self.device)\n",
    "\n",
    "        return encode_list\n",
    "\n",
    "    def decode(self, sentences):\n",
    "        decode_list = []\n",
    "\n",
    "        if not isinstance(sentences[0], list):\n",
    "            # 解码过程中, 如果只有单列表, 例如 [1, 2, 3] 则需要额外嵌套一层列表\n",
    "            # 默认的解码都是一个批次的, 因此是双层列表嵌套\n",
    "            sentences = [sentences]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            decode_list.append([self.index2token[deco] for deco in sentence])\n",
    "        return decode_list\n",
    "\n",
    "    def generate(self, tokenList: Literal[Literal]):\n",
    "\n",
    "        def single(tokenList):\n",
    "            str_to_return = ''\n",
    "            for char in tokenList:\n",
    "                str_to_return += char\n",
    "            return str_to_return\n",
    "\n",
    "        return [single(token_list) for token_list in tokenList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75561ef2-5cd5-42c5-9e54-a2abd89c4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(max_len=max_len, sos_token='$', padding_token='&', eos_token='#', device=device)\n",
    "tokenizer.from_pretrained(\"./VOCABULARY.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea17f37f-d19d-4c1b-8acf-2031bbd9919e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 53)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "padding_idx = tokenizer.padding_index  # padding token 的序列号\n",
    "(vocab, padding_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a005894-f6fb-430a-8839-aa04112e650a",
   "metadata": {},
   "source": [
    "### 模型设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19dd15d9-bf0d-46e8-af63-dfcbdd8faf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 嵌入部分\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab, max_len, d_model, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        1. 嵌入部分包括词嵌入和位置编码, 二者相加 -> dropout -> 作为编码器或解码器的输入。\n",
    "        2. 细节: 词嵌入会设置导数, 位置编码的索引张量不设置导数\n",
    "        :param vocab: 单词表的数量\n",
    "        :param max_len: 一句话的最大 token 长度\n",
    "        :param d_model: 词嵌入向量维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 张量存放设备\n",
    "        :param dtype: 张量数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 词嵌入部分\n",
    "        self.word_embedding = nn.Embedding(num_embeddings=self.vocab, embedding_dim=self.d_model)\n",
    "\n",
    "        # 位置编码部分: (max_len, d_model) 的二维张量, 不需要导数\n",
    "        self.position_embedding_map = torch.zeros(size=(self.max_len, self.d_model))\n",
    "        self.position_embedding_map.requires_grad = False\n",
    "        # 设置奇数维度和偶数维度的索引列表\n",
    "        odd, even = torch.arange(1, self.d_model, 2), torch.arange(0, self.d_model, 2)\n",
    "        # 设置一个句子 token 的全部索引\n",
    "        # 这里 \"unsqueeze(1)\" 的作用是让句子 token 的位置索引可以广播\n",
    "        ########################################################################################\n",
    "        # 回忆: pytorch 的张量基本运算是按元素位置计算的, 因此两个相同 shape 的张量结果返回的也是相同 shape\n",
    "        # 例如: 两个张量的维度都是 (4.) , 进行基本运算的结果就是 (4.)\n",
    "        # 例如: 一个张量的维度是 (4.) , 另一个维度是 (4, 1) 那么 (4, 1) 会广播成 (4, 4) 再做运算\n",
    "        # 分析: pos 的原本张量维度是 (self.max_len.) , even 和 odd 都是 (256.) 直接做运算会报错\n",
    "        # 分析: 原本情况 pos 和 even 都被视为向量, 但是二者维度不匹配, 因此无法计算\n",
    "        # 分析: pos 最后扩充一个维度时, (self.max_len, 1) , pos 会在最后这个 1 维度广播机制重复 256 次\n",
    "        # 分析: 再将广播机制后的 (self.max_len, 256) 与 even 或者 odd 做运算\n",
    "        ########################################################################################\n",
    "        pos = torch.arange(0, self.max_len, 1).unsqueeze(1)\n",
    "        # 根据 odd, even 和 pos 填充 self.position_embedding_map 张量内的元素\n",
    "        self.position_embedding_map[:, even] = torch.sin(pos / (1e4 ** (even / self.d_model)))\n",
    "        self.position_embedding_map[:, odd] = torch.cos(pos / (1e4 ** (even / self.d_model)))\n",
    "\n",
    "        # 正则化部分\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "        # 对初始化好的 self.word_embedding 和 self.position_embedding_map 进行数据类型和设备设置\n",
    "        self.word_embedding.to(device=self.device, dtype=self.dtype)\n",
    "        self.position_embedding_map.to(device=self.device, dtype=self.dtype)\n",
    "        self.dropout.to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        对输入的离散稀疏 token 的编号计算得到连续稠密 embedding 向量\n",
    "        :param x: 输入 tokens 的编号序列\n",
    "        :return: 输出 embedding 张量\n",
    "        \"\"\"\n",
    "        # batch_size 一批训练数据内包含句子的数量\n",
    "        # max_len 一批训练数据内单句最大长度, 且 max_len <= 位置编码初始化设定的长度\n",
    "        batch_size, max_len = x.shape\n",
    "\n",
    "        # 词嵌入\n",
    "        word_embedding = self.word_embedding(x)\n",
    "        # 位置编码\n",
    "        # debug: 最初维度要扩充一个维度, 以实现广播机制\n",
    "        # debug: position_encoding 存储在 cpu 上, 还要将其转到 cuda 上\n",
    "        position_encode = self.position_embedding_map[:max_len, :].unsqueeze(0).to(device=self.device, dtype=self.dtype)\n",
    "        # 两者相加再 dropout 正则化\n",
    "        encode = self.dropout(word_embedding + position_encode)\n",
    "\n",
    "        return encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2dfc20-fe2b-4a6e-939b-fb814cc5ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 层归一化算法\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, dtype, device, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        :param d_model: 模型嵌入维度\n",
    "        :param dtype: 数据类型\n",
    "        :param device: 显卡设备\n",
    "        :param epsilon: 设置的一个很小很小的数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.epsilon = epsilon\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        # 初始化可以更新的全 0 全 1 张量参数\n",
    "        self.A = nn.Parameter(torch.ones(self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "        self.B = nn.Parameter(torch.zeros(self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. 不计算无偏的, 也就是方差公式除以 N 而不是 N-1\n",
    "        # 2. keepdim=True, 保留原有维度, 便于进行广播机制\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        var = torch.var(x, dim=-1, unbiased=False, keepdim=True)\n",
    "        x = self.A * (x - mean) / (var + self.epsilon) + self.B\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8242231b-c501-4300-9333-6a76f2eb4d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 编码器部分\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, max_len, d_model, n_heads, d_proj, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        :param max_len: 单句最大长度\n",
    "        :param d_model: 词嵌入向量维度\n",
    "        :param n_heads: 注意力头数\n",
    "        :param d_proj: 投影层的维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 张量存放设备\n",
    "        :param dtype: 张量数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_proj = d_proj\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 计算出每个头的子空间维度\n",
    "        self.d_per_head = int(self.d_model / self.n_heads)\n",
    "\n",
    "        # 初始化 query key value 的线性投射层\n",
    "        self.Wq = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wk = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wv = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化头合并的投射层, 以及前馈神经网络 FFNN\n",
    "        self.Wc = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wf = nn.Sequential(nn.Linear(self.d_model, self.d_proj),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(self.d_proj, self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化正则化\n",
    "        self.Dropout = nn.Dropout(self.dropout).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化层归一化\n",
    "        self.layerNorm = LayerNorm(self.d_model, self.dtype, self.device)\n",
    "\n",
    "        # 对所有线性投射层的权重进行恺明初始化\n",
    "        torch.nn.init.kaiming_uniform_(self.Wq.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wk.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wv.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wc.weight)\n",
    "        for layer in self.Wf:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, input_content: Tuple[Tensor, ByteTensor]):\n",
    "        \"\"\"\n",
    "        :param input_content: 输入内容的元组, 包含两个元素: 一个是输入张量, 另一个是注意力掩码\n",
    "        \"\"\"\n",
    "        x, mask = input_content\n",
    "\n",
    "        # batch_size 一个批次的句子数量\n",
    "        # max_len 句子的最大 token 数量\n",
    "        # d_model 嵌入维度\n",
    "        batch_size, max_len, d_model = x.shape\n",
    "\n",
    "        # x 分别当做 query key value 输入线性投射层\n",
    "        # x_q, x_k, x_v: (batch_size, max_len, d_model)\n",
    "        x_q, x_k, x_v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "\n",
    "        # 分头, 并将头分出来\n",
    "        # x_q, x_k, x_v: (batch_size, n_heads, max_len, d_per_head)\n",
    "        x_q = x_q.view(batch_size, max_len, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_k = x_k.view(batch_size, max_len, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_v = x_v.view(batch_size, max_len, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 注意力机制计算\n",
    "        # 做乘法注意力 temp: (batch_size, n_heads, max_len, max_len)\n",
    "        temp = x_q @ x_k.permute(0, 1, 3, 2)\n",
    "        # 除以缩放算子\n",
    "        temp /= math.sqrt(self.d_per_head)\n",
    "        ########################################################################################\n",
    "        # 其中 mask 必须是一个 ByteTensor, shape 必须和 a 一样, 且元素只能是 0 或者 1 .\n",
    "        # 将 mask 中为 1 的元素所在的索引, 在 a 中相同的的索引处替换为 value, mask value 必须同为 tensor\n",
    "        # 这里的 mask 被掩码的是 1, 没被掩码的是 0\n",
    "        ########################################################################################\n",
    "        # 进行自编码器掩码操作\n",
    "        temp.masked_fill(mask, -1 * torch.inf)\n",
    "        # 进行 softmax 归一化, dim=-1 表示只对最后的维度, 就是嵌入维度做归一化\n",
    "        # attention: (batch_size, n_heads, max_len, max_len)\n",
    "        attention = torch.softmax(temp, dim=-1)\n",
    "        # 将注意力分数乘以 value 张量\n",
    "        # value: (batch_size, n_heads, max_len, d_per_head)\n",
    "        value = attention @ x_v\n",
    "        # 将每个头合并\n",
    "        # 若在维度变换后还需要进行 reshape 操作的话, 需要在后面加 contiguous() 保持连续 \n",
    "        # total_value: (batch_size, max_len, d_model)\n",
    "        total_value = value.permute(0, 2, 1, 3).contiguous().reshape((batch_size, max_len, d_model))\n",
    "        # 输入头合并投影层, 输出表示自注意力模块结束\n",
    "        # total_value_: (batch_size, max_len, d_model)\n",
    "        total_value_ = self.Wc(total_value)\n",
    "        # 马上进行正则化\n",
    "        total_value_ = self.Dropout(total_value_)\n",
    "        # 进行残差连接和层归一化\n",
    "        last = self.layerNorm(x + total_value_)\n",
    "        return (last, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e29bb12-37b0-4b50-a3e8-0c7cfccce475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 解码器部分\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, max_len, d_model, n_heads, d_proj, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        :param max_len: 单句最大长度\n",
    "        :param d_model: 词嵌入向量维度\n",
    "        :param n_heads: 注意力头数\n",
    "        :param d_proj: 投影层的维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 张量存放设备\n",
    "        :param dtype: 张量数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_proj = d_proj\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # 计算出每个头的子空间维度\n",
    "        self.d_per_head = int(self.d_model / self.n_heads)\n",
    "\n",
    "        # 初始化 query key value 的线性投射\n",
    "        self.Wq = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wk = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wv = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化头合并的投射以及前馈神经网络\n",
    "        self.Wc = nn.Linear(self.d_model, self.d_model).to(device=self.device, dtype=self.dtype)\n",
    "        self.Wf = nn.Sequential(nn.Linear(self.d_model, self.d_proj),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(self.d_proj, self.d_model)).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化正则化\n",
    "        self.Dropout = nn.Dropout(self.dropout).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        # 初始化层归一化\n",
    "        self.layerNorm = LayerNorm(self.d_model, self.dtype, self.device)\n",
    "\n",
    "        # 对所有线性投射层的权重进行恺明初始化\n",
    "        torch.nn.init.kaiming_uniform_(self.Wq.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wk.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wv.weight)\n",
    "        torch.nn.init.kaiming_uniform_(self.Wc.weight)\n",
    "        for layer in self.Wf:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, input_content: Tuple[Tensor, Tensor, Tensor, ByteTensor, ByteTensor]):\n",
    "        \"\"\"\n",
    "        :param input_content: 输入内容的元组, 包含五个元素: 三个是 query key 和 value , 另外两个是解码注意力掩码和编码解码掩码\n",
    "        \"\"\"\n",
    "        q, k_, v_, cross_mask, decode_mask = input_content\n",
    "\n",
    "        k = copy.copy(k_)\n",
    "        v = copy.copy(v_)\n",
    "\n",
    "        batch_size_q, max_len_q, d_model_q = q.shape\n",
    "        batch_size_k, max_len_k, d_model_k = k.shape\n",
    "\n",
    "        # x 分别当做 query key value 输入线性投射层\n",
    "        # x_q, x_k, x_v: (batch_size, max_len, d_model)\n",
    "        x_q, x_k, x_v = self.Wq(q), self.Wk(q), self.Wv(q)\n",
    "\n",
    "        # 分头, 并将头分出来\n",
    "        # x_q, x_k, x_v: (batch_size, n_heads, max_len, d_per_head)\n",
    "        x_q = x_q.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_k = x_k.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        x_v = x_v.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 注意力机制计算\n",
    "        # 做乘法注意力 temp: (batch_size, n_heads, max_len, max_len)\n",
    "        temp = x_q @ x_k.permute(0, 1, 3, 2)\n",
    "        # 除以缩放算子\n",
    "        temp /= math.sqrt(self.d_per_head)\n",
    "        ########################################################################################\n",
    "        # 其中 mask 必须是一个 ByteTensor, shape 必须和 a 一样, 且元素只能是 0 或者 1 .\n",
    "        # 将 mask 中为 1 的元素所在的索引, 在 a 中相同的的索引处替换为 value, mask value 必须同为 tensor\n",
    "        # 这里的 mask 被掩码的是 1, 没被掩码的是 0\n",
    "        ########################################################################################\n",
    "        # 进行自编码器掩码操作\n",
    "        temp.masked_fill(decode_mask, -1 * torch.inf)\n",
    "        # 进行 softmax 归一化, dim=-1 表示只对最后的维度, 就是嵌入维度做归一化\n",
    "        # attention: (batch_size, n_heads, max_len, max_len)\n",
    "        attention = torch.softmax(temp, dim=-1)\n",
    "        # 将注意力分数乘以 value 张量\n",
    "        # value: (batch_size, n_heads, max_len, d_per_head)\n",
    "        value = attention @ x_v\n",
    "        # 将每个头合并\n",
    "        # total_value: (batch_size, max_len, d_model)\n",
    "        total_value = value.permute(0, 2, 1, 3).contiguous().reshape((batch_size_q, max_len_q, d_model))\n",
    "        # 输入头合并投影层, 输出表示自注意力模块结束\n",
    "        # total_value_: (batch_size, max_len, d_model)\n",
    "        total_value_ = self.Wc(total_value)\n",
    "        # 马上进行正则化\n",
    "        total_value_ = self.Dropout(total_value_)\n",
    "        # 进行残差连接和层归一化\n",
    "        last = self.layerNorm(q + total_value_)\n",
    "\n",
    "        # --------------------------------------------\n",
    "        last_ = copy.copy(last)\n",
    "        last_ = last_.view(batch_size_q, max_len_q, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        k = k.view(batch_size_k, max_len_k, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "        v = v.view(batch_size_k, max_len_k, self.n_heads, self.d_per_head).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 注意力机制计算\n",
    "        temp = last_ @ k.permute(0, 1, 3, 2)\n",
    "        # 除以缩放算子\n",
    "        temp /= math.sqrt(self.d_per_head)\n",
    "        # 进行编码-解码器掩码操作\n",
    "        temp.masked_fill(cross_mask, -1 * torch.inf)\n",
    "        # 进行 softmax 归一化\n",
    "        attention = torch.softmax(temp, dim=-1)\n",
    "        # 将注意力分数乘以 value 张量\n",
    "        value = attention @ v\n",
    "        # 将每个头合并\n",
    "        total_value = value.permute(0, 2, 1, 3).contiguous().reshape((batch_size_q, max_len_q, d_model))\n",
    "        # 输入头合并投影层, 输出表示互注意力模块结束\n",
    "        total_value_ = self.Wc(total_value)\n",
    "        # 马上进行正则化\n",
    "        total_value_ = self.Dropout(total_value_)\n",
    "        # 进行残差连接和层归一化\n",
    "        last_decode = self.layerNorm(last + total_value_)\n",
    "\n",
    "        return (last_decode, k_, v_, cross_mask, decode_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "532ed498-999b-406d-b6d6-b53112077d79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 投射到词汇表\n",
    "class ProjVocab(nn.Module):\n",
    "    def __init__(self, vocab, d_model, d_proj, dropout, device, dtype):\n",
    "        \"\"\"\n",
    "        :param vocab: 词汇表数量\n",
    "        :param d_model: 模型嵌入维度\n",
    "        :param d_proj: 线性投射层维度\n",
    "        :param dropout: 正则化率\n",
    "        :param device: 显卡设备\n",
    "        :param dtype: 数据类型\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.d_proj = d_proj\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.projVocab = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_proj),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_proj, self.vocab)\n",
    "        ).to(device=self.device, dtype=self.dtype)\n",
    "\n",
    "        for layer in self.projVocab:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                torch.nn.init.kaiming_uniform_(layer.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projVocab(x)\n",
    "        # softmax 归一化处理, 含义是 \"预测出来的, 在词汇表上每个 token 出现的概率\"\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9edb9d-795a-48ec-a6d9-88c025074302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# 汇总成 Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_len, d_model, n_heads, n_layers, d_proj, vocab, dropout, device, dtype, epsilon=1e-7):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_proj = d_proj\n",
    "        self.vocab = vocab\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # 每个模块类实例的时候都进行了权重初始化, 设备和数据类型的声明\n",
    "        # 初始化单个编码器和解码器\n",
    "        single_encoder = Encoder(self.max_len, self.d_model, self.n_heads, self.d_proj, self.dropout, self.device,\n",
    "                                 self.dtype)\n",
    "        single_decoder = Decoder(self.max_len, self.d_model, self.n_heads, self.d_proj, self.dropout, self.device,\n",
    "                                 self.dtype)\n",
    "\n",
    "        # 初始化编码部分和解码部分的嵌入层\n",
    "        self.embedding = Embedding(self.vocab, self.max_len, self.d_model, self.dropout, self.device, self.dtype)\n",
    "\n",
    "        # 使用列表解包的方法构建整个网络\n",
    "        # 注意: nn.Sequential 构建的网络只允许单个变量输入模型中, 因此在编码器和解码器中进行了打包和解包操作\n",
    "        self.encoders = nn.Sequential(\n",
    "            *[single_encoder for _ in range(self.n_layers)]\n",
    "        )\n",
    "        self.decoders = nn.Sequential(\n",
    "            *[single_decoder for _ in range(self.n_layers)]\n",
    "        )\n",
    "\n",
    "        # 初始化线性投射层\n",
    "        self.projVocab = ProjVocab(self.vocab, self.d_model, self.d_proj, self.dropout, self.device, self.dtype)\n",
    "\n",
    "    def forward(self, src_seq, trg_seq):\n",
    "        \"\"\"\n",
    "        训练过程的 transformer 的前向推理\n",
    "        :param src_seq: 源序列\n",
    "        :param trg_seq: 目标序列\n",
    "        :return: 每个句子每个 token 的下一个预测的 token \n",
    "        \"\"\"\n",
    "        # 先获得编码器掩码, 解码器掩码和编码-解码掩码\n",
    "        self_mask = self.make_mask(src_seq, src_seq, \"encoder\")\n",
    "        cross_mask = self.make_mask(trg_seq, src_seq, \"encoder-decoder\")\n",
    "        decoder_mask = self.make_mask(trg_seq, trg_seq, \"decoder\")\n",
    "\n",
    "        # 计算源序列嵌入和目标序列嵌入\n",
    "        en_emb = self.embedding(src_seq)\n",
    "        de_emb = self.embedding(trg_seq)\n",
    "\n",
    "        # 将源序列嵌入和编码器掩码送入编码器计算源序列的编码信息\n",
    "        encodes_tuple = self.encoders((en_emb, self_mask))\n",
    "        # 从计算结果的包中得到编码\n",
    "        encodes, _ = encodes_tuple\n",
    "        # 将目标序列的嵌入, 两个编码 (分别做 key 和 value), 编码-解码掩码 和 解码器掩码送入解码器计算每句话每个 token 的下一个预测\n",
    "        decodes = self.decoders((de_emb, encodes, encodes, cross_mask, decoder_mask))\n",
    "        # 从计算结果的包中得到解码\n",
    "        last_decode, _, _, _, _ = decodes\n",
    "        # 将解码内容送入投射层中获得在词汇表中每个 token 的预测概率\n",
    "        vocab_pos = self.projVocab(last_decode)\n",
    "        return vocab_pos\n",
    "\n",
    "    def make_mask(self, q: Tensor, k: Tensor, type: Literal[\"encoder\", \"encoder-decoder\", \"decoder\"]):\n",
    "        \"\"\"\n",
    "        这个类方法用于构建编码器掩码, 解码器掩码和编码-解码掩码\n",
    "        :param q: query 张量\n",
    "        :param k: key 张量\n",
    "        :param type: 选择是编码器掩码, 解码器掩码 和编码-解码掩码\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        max_len_q = q.shape[1]  # 获得 query 和 key 的每一句话的最大 token 长度\n",
    "        max_len_k = k.shape[1]\n",
    "\n",
    "        # qMask: (batch_size, max_len_q)\n",
    "        qMask = q.ne(padding_idx)  # 过滤掉被 padding 的 token\n",
    "        # qMask: (batch_size, 1, max_len_q, 1)\n",
    "        qMask = qMask.unsqueeze(1).unsqueeze(3)\n",
    "        # qMask: (batch_size, 1, max_len_q, max_len_k)\n",
    "        qMask = qMask.repeat(1, 1, 1, max_len_k)\n",
    "\n",
    "        # kMask: (batch_size, max_len_k)\n",
    "        kMask = k.ne(padding_idx)\n",
    "        # kMask: (batch_size, 1, 1, max_len_k)\n",
    "        kMask = kMask.unsqueeze(1).unsqueeze(2)\n",
    "        # kMask: (batch_size, 1, max_len_q, max_len_k)\n",
    "        kMask = kMask.repeat(1, 1, max_len_q, 1)\n",
    "\n",
    "        Mask = qMask & kMask\n",
    "\n",
    "        # 如果是解码器注意力时, 需要设置一个同大小的下三角掩码, 然后做与运算\n",
    "        if type == \"decoder\":\n",
    "            trigl = torch.tril(torch.ones_like(Mask))\n",
    "            Mask &= trigl\n",
    "\n",
    "        return Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91a2436e-3519-40b9-98ac-a901560b54e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(max_len=max_len,\n",
    "                          d_model=d_model,\n",
    "                          n_heads=n_heads,\n",
    "                          n_layers=n_layers,\n",
    "                          d_proj=d_proj,\n",
    "                          vocab=vocab,\n",
    "                          dropout=drop_prob,\n",
    "                          device=device,\n",
    "                          dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb30ae2-dc33-428c-989f-331d1836d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 7.49M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in transformer.parameters()])\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "036dd613-24b4-44e3-8314-d33855f7729b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(\n",
       "    (word_embedding): Embedding(55, 512)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoders): Sequential(\n",
       "    (0): Encoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (decoders): Sequential(\n",
       "    (0): Decoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (1): Decoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "    (2): Decoder(\n",
       "      (Wq): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wk): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wv): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (Wf): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (Dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layerNorm): LayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (projVocab): ProjVocab(\n",
       "    (projVocab): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=2048, out_features=55, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b558e-6479-4ee6-833b-ee4daee30e9d",
   "metadata": {},
   "source": [
    "### 数据集设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2cea86f-9e6a-432a-8da5-aa44c8e931ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, src_dataset_path, trg_dataset_path, batch_size, shuffle=True):\n",
    "        self.src_dataset_path = src_dataset_path\n",
    "        self.trg_dataset_path = trg_dataset_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Load data from files\n",
    "        self.data1 = self._load_data(src_dataset_path)\n",
    "        self.data2 = self._load_data(trg_dataset_path)\n",
    "\n",
    "        # Shuffle data if required\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.data1)\n",
    "            random.shuffle(self.data2)\n",
    "\n",
    "        # Initialize indices for batching\n",
    "        self.index1 = 0\n",
    "        self.index2 = 0\n",
    "\n",
    "        # 计算遍历完整个数据集需要的循环次数\n",
    "        self.step = (len(self.data1) // self.batch_size) + 1\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return [line.strip() for line in file]\n",
    "\n",
    "    def _get_next_batch(self, data, index):\n",
    "        if index + self.batch_size > len(data):\n",
    "            batch = data[index:] + data[:(index + self.batch_size) % len(data)]\n",
    "            index = (index + self.batch_size) % len(data)\n",
    "        else:\n",
    "            batch = data[index:index + self.batch_size]\n",
    "            index += self.batch_size\n",
    "        return batch, index\n",
    "\n",
    "    def get_batch(self):\n",
    "        batch1, self.index1 = self._get_next_batch(self.data1, self.index1)\n",
    "        batch2, self.index2 = self._get_next_batch(self.data2, self.index2)\n",
    "        return batch1, batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b212257-5649-4499-b47e-4a091a6c38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\"./datasets/src_string.txt\", \"./datasets/trg_string.txt\", batch_size=batch_size, shuffle=True)\n",
    "dataloader_valid = DataLoader(\"./datasets/src_valid.txt\", \"./datasets/trg_valid.txt\", batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7392549-7454-4f7f-8e86-8e8626e1dee0",
   "metadata": {},
   "source": [
    "### 数据集制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fee2cb9e-699a-4611-a24c-50e91132c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_strings(n, l, src, trg):\n",
    "    total_tokens = 0\n",
    "\n",
    "    with open(src, 'w') as srcFP:\n",
    "        with open(trg, 'w') as trgFP:\n",
    "            for _ in range(n):\n",
    "                length = random.randint(1, l)\n",
    "                random_string = ''.join(random.choices(string.ascii_letters, k=length))\n",
    "                srcFP.write(random_string + '\\n')\n",
    "                reversed_string = random_string[::-1]\n",
    "                trgFP.write(reversed_string + '\\n')\n",
    "                total_tokens += len(random_string)\n",
    "\n",
    "    print(f\"Generated {n} strings with total {total_tokens} tokens and saved to {src} and {trg}\")\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54a30339-32a2-4f22-8fce-fb1a1f7780d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5000 strings with total 320141 tokens and saved to ./datasets/src_string.txt and ./datasets/trg_string.txt\n"
     ]
    }
   ],
   "source": [
    "total_tokens = generate_strings(n, l, src, trg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd1b1b-bd57-4fe8-a249-15be8ed8c280",
   "metadata": {},
   "source": [
    "### 评估组件初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb144e3b-7107-498b-9d1d-87db3152c879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(ref, candi):\n",
    "\n",
    "    def find_comm_str(strs: list):\n",
    "        sort_str = sorted(strs, key=len)\n",
    "        max_comm = set()\n",
    "        min_len = len(sort_str[0])\n",
    "        # 从最长向最短匹配\n",
    "        for i in range(min_len, 0, -1):\n",
    "            # 每次最多可以分出几个字符串,循环匹配的次数\n",
    "            for j in range(min_len - i + 1):\n",
    "                math_str = sort_str[0][j:j + i]\n",
    "                # 查看每次匹配的字符串\n",
    "                # print(j,j+i,math_str)\n",
    "                flag = True\n",
    "                # 假设有大于2个字符串需要参加匹配\n",
    "                for big_str in sort_str[1:]:\n",
    "                    if math_str not in big_str:\n",
    "                        flag = False\n",
    "                if flag:\n",
    "                    max_comm.add(math_str)\n",
    "            if len(max_comm) > 0:\n",
    "                break\n",
    "        return max_comm\n",
    "\n",
    "    # rouge = Rouge()\n",
    "    # scores = rouge.get_scores(candi, ref)\n",
    "    # rouge_1 = scores[0]['rouge-1']['f']\n",
    "    # rouge_2 = scores[0]['rouge-2']['f']\n",
    "    # rouge_l = scores[0]['rouge-l']['f']\n",
    "    # return rouge_1, rouge_2, rouge_l\n",
    "    max_comm = find_comm_str([candi, ref])\n",
    "    return len(max_comm) / len(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d60d1c53-2dc2-4a76-b5e2-aab0c21a0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    rouge_per_batch = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(dataloader.step):\n",
    "            src, trg = dataloader.get_batch()\n",
    "\n",
    "            src, trg = tokenizer.encode(src, return_tensor=True), tokenizer.encode(trg, return_tensor=True)\n",
    "\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg_ = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg_)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            total_rouge = []\n",
    "            for j in range(batch_size):\n",
    "                # try:\n",
    "                trg_words = tokenizer.decode(trg[j, :].data.cpu().numpy().tolist())\n",
    "                trg_words = tokenizer.generate(trg_words)[0]\n",
    "\n",
    "                output_words = output[j].softmax(dim=-1).max(dim=-1)[1]\n",
    "                output_words = tokenizer.decode(output_words.data.cpu().numpy().tolist())\n",
    "                output_words = tokenizer.generate(output_words)[0]\n",
    "\n",
    "                rouge_scores = calculate_rouge(output_words, trg_words)\n",
    "                total_rouge.append(rouge_scores)\n",
    "                # except:\n",
    "                #     pass\n",
    "\n",
    "            rouge_scores = sum(total_rouge) / len(total_rouge)\n",
    "            rouge_per_batch.append(rouge_scores)\n",
    "\n",
    "    return validation_loss / dataloader.step, sum(rouge_per_batch) / len(rouge_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66125313-0451-4f25-a517-c114f5e48d9d",
   "metadata": {},
   "source": [
    "### 训练组件初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64dce5d7-6a45-4419-9653-2ae1e7826cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(params=transformer.parameters(),\n",
    "                 lr=init_lr,\n",
    "                 weight_decay=weight_decay,\n",
    "                 eps=adam_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41a82b09-a1fb-4044-be7e-a6410b56b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 verbose=True,\n",
    "                                                 factor=factor,\n",
    "                                                 patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d564c827-60b0-44b4-b81c-4e820dffd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "# torch.nn.CrossEntropyLoss(input, target) 中的标签 target 使用的不是 one-hot 形式，而是类别的序号。\n",
    "# 形如 target = [1, 3, 2] 表示3个样本分别属于第1类、第3类、第2类。（单标签多分类问题）\n",
    "# torch.nn.CrossEntropyLoss(input, target) 的 input 是没有归一化的每个类的得分，\n",
    "# 而不是softmax之后的分布。\n",
    "#####################################################################\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.padding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd76ef20-91c4-4ec0-9ff9-da2cd0204d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, dataloader_valid, num_epoch, optimizer, criterion, clip):\n",
    "    # 让模型进入训练模式\n",
    "    model.train()\n",
    "\n",
    "    # 初始化三个记录指标: epoch_record 训练损失, valid_record 验证集损失, rouge_recoed 验证集精度\n",
    "    epoch_record = []\n",
    "    valid_record = []\n",
    "    rouge_recoed = []\n",
    "\n",
    "    # 顺便记录一下时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i in tqdm.notebook.tqdm(range(dataloader.step)):\n",
    "            src, trg = dataloader.get_batch()\n",
    "\n",
    "            src, trg = tokenizer.encode(src, return_tensor=True), tokenizer.encode(trg, return_tensor=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output_reshape, trg)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            # print('step :', round((i / dataloader.step) * 100, 2), '% , loss :', loss.item())\n",
    "\n",
    "        print(\"=====================================================================\")\n",
    "        print(f\"EPOCH: {epoch} LOSS: {epoch_loss / dataloader.step}\")\n",
    "        print(\"=====================================================================\")\n",
    "\n",
    "        valid_loss, rouge_l = evaluation(model, dataloader_valid, criterion)\n",
    "\n",
    "        epoch_record.append(epoch_loss / dataloader.step)\n",
    "        valid_record.append(valid_loss)\n",
    "        rouge_recoed.append(rouge_l)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    with open(\"./records/train.txt\", \"w\") as fp:\n",
    "        for data in epoch_record:\n",
    "            fp.write(str(data) + '\\n')\n",
    "\n",
    "    with open(\"./records/valid.txt\", \"w\") as fp:\n",
    "        for data in valid_record:\n",
    "            fp.write(str(data) + '\\n')\n",
    "\n",
    "    with open(\"./records/rouge.txt\", \"w\") as fp:\n",
    "        for data in rouge_recoed:\n",
    "            fp.write(str(data) + '\\n')\n",
    "\n",
    "    return {\"epoch_record\": epoch_record,\n",
    "            \"valid_record\": valid_record,\n",
    "            \"rouge_recoed\": rouge_recoed,\n",
    "            \"time_usage\": end_time - start_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dba1b80e-e16c-4784-864d-11832d9aadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epoch_losses(records, name=\"results.png\"):\n",
    "\n",
    "    # 假设你的三个数据列表如下：\n",
    "    epoch_record = records[\"epoch_record\"]\n",
    "    valid_record = records[\"valid_record\"]\n",
    "    rouge_recoed = records[\"rouge_recoed\"]\n",
    "\n",
    "    fig, axs = plt.subplots(3, figsize=(8, 12)) # 创建一个包含3个子图的图形\n",
    "    \n",
    "    # 在每个子图上绘制一个数据集，并添加标题\n",
    "    axs[0].plot(epoch_record)\n",
    "    axs[0].set_title('Loss for epoch_record')\n",
    "    \n",
    "    axs[1].plot(valid_record)\n",
    "    axs[1].set_title('Loss for valid_record')\n",
    "    \n",
    "    axs[2].plot(rouge_recoed)\n",
    "    axs[2].set_title('rouge_recoed')\n",
    "\n",
    "    plt.savefig(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295bc6e-129c-411a-ab50-e8d87797bcbe",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdda1e6-e6bb-4e3e-8dee-59ca9f2d448d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765a93a117e74cf3852f38f67b18ce06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 0 LOSS: 5.292896842956543\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6229c70fe84a5ba90ce14b45d53dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 1 LOSS: 4.253975689411163\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9b893aa7994eb2a00c7edf3819b7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 2 LOSS: 4.052235758304596\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f160852ad8472d8eca417cd944738d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 3 LOSS: 4.021782755851746\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67236ec7e354c67a1cd404102360a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 4 LOSS: 4.010021495819092\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634e1ea03e74439fa16f9819f5c70164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 5 LOSS: 4.002526116371155\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f870c366878a4979b4af2dbcfe3da445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 6 LOSS: 3.9967622220516206\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55fba5e046645d4a9ceb92567506a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 7 LOSS: 3.9923280835151673\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639c797a03a249bd83832fb56a2ac975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 8 LOSS: 3.9887291550636292\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d586eb84eed847f18ad1edc5a9692333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 9 LOSS: 3.985332137346268\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a897df70cba24e89944e05ca3fcd55a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 10 LOSS: 3.9825436890125276\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8414888e93a6405fac96fbe589bb2e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 11 LOSS: 3.9800866544246674\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80377c33f2a7492787c71dfa8a6eb235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 12 LOSS: 3.9776349544525145\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acac0a426c6e4101b69478a0306dedc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "EPOCH: 13 LOSS: 3.9752903282642365\n",
      "=====================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689b81c909d447fc8d5550ccfc151fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "records = train(model=transformer,\n",
    "            dataloader=dataloader,\n",
    "            dataloader_valid=dataloader_valid,\n",
    "            num_epoch=epoch,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            clip=clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500c0ed-8ea1-4a3d-ad3e-590290774535",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epoch_losses(records, name=save_plot_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502be09-eba3-4305-8998-da5baa9c466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"训练用时: {}\".format(records[\"time_usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9fb545-1878-4ddb-b4fc-ff212aa1aa73",
   "metadata": {},
   "source": [
    "### 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e2c7d-a5d5-411d-9ed9-01e88207ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path: str):\n",
    "    # 保存和加载整个模型\n",
    "    torch.save(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74960bc8-93bf-4162-8971-334eff54593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(transformer, \"my_transformer.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
